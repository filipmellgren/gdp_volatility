---
title: "GDP Volatility"
author: "Filip Mellgren"
date: '2020-05-09'
output:
  html_document:
    df_print: kable
  pdf_document: default
---
bibliography: bibliography.bib

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
```

```{r,include=FALSE}
library(rmgarch) # https://cran.r-project.org/web/packages/rmgarch/rmgarch.pdf
# More helpful slides on the above package: https://faculty.washington.edu/ezivot/econ589/DCCgarchPowerpoint.pdf
library(rio)
library(tidyverse)
library(ggplot2); theme_set(theme_minimal())
library(lubridate)
library(rmgarch)
library(xts)
library(patchwork)
library(zoo) # In tidyverse already?
library(mFilter)
```

```{r parameters}
start_date <- as.POSIXct("1961-01-01")
end_date <- as.POSIXct("2019-12-01")
countries <- c("FRA", "GBR", "DEU", "ESP")
n_countries <- length(countries)
```

# Task
data analysis icluding original discussion of a data set with a clear final goal

# Notes
Find a stationary process, returns are usually stationary

# Analysis notes
This analysis was inspired by: https://ro.uow.edu.au/cgi/viewcontent.cgi?article=1280&context=aabfj
https://www.jstor.org/stable/pdf/27647202.pdf?refreqid=excelsior%3Ada7c65572902730eb32916bc6ff4825c

Two additional params on top of CCC. Very parsimonous, not exploding in number of assets. Assume  nassume same params for every pairs.

Correlation is allowed to be time variant in the DCC (dynamic conditional correlation)

Problems: assumption that two params are the same for al pairs can be a bit relaxed. Assets of similar sectors can be shared for instance. 

Extensions: PCA, factor GARCH. Dimensioanlity reduction.

Explanatory paper of the library rmgarch by the author: https://cran.r-project.org/web/packages/rmgarch/vignettes/The_rmgarch_models.pdf

Paper on gdp synchroisation:
https://ec.europa.eu/eurostat/documents/3888793/7572028/KS-TC-16-010-EN-N.pdf/969fe12f-cc19-4447-81f0-a5c95265798a

Youtube video: https://www.youtube.com/watch?v=8VXmRl5gzEU&t=27s

A paper using DCC on turkey for finding synchronisation between GDP and stock marketets: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=Analyzing+the+synchronization+between+the+financial+and+business+cycles+in+Turkey&btnG=

Stock Watson 2005 is authoriative: UNDERSTANDING CHANGES IN INTERNATIONAL BUSINESS CYCLE DYNAMICS
P. 974 about the data used to analyse volatitlity:  "Let yt be the quarterly GDP growth
at an annual rate". They look att individual countries and not conditional correlation between countries.

Questions:
* Do we analyse the raw growth data or some HP filtered data? If so, what component?
- Longterm or short term being of interest might address this
* How can we select the right DCC specification?

# Introduction
Following the introduction of the common currency among EMU countries, it is of importance that business cycles are synchronised in order to make monetary policy widely effective. 

To investigate the degree of business cycle synchronisation, I look at the development of correlations across coutries over time using a dynamic conditional correlation approach CITE Engle (2002), a widely used benchmark model that is flexible without requiring too many parameters to be estimated.

# Data
The data set was downloaded from the OECD https://data.oecd.org/gdp/quarterly-gdp.htm
cite: "OECD (2020), Quarterly GDP (indicator). doi: 10.1787/b86d1fc8-en (Accessed on 15 May 2020)"

The data set contains information on seasonally adjsuted percentage change in constant prices gross domestic product. The frequency of the data is quarterly and each value denotes the percentage change from one quarter to the next. Included in the data set are 25 EU countries, including the United Kingdom, but excluding Cyprus, Croatia, and Malta. The series start in 1955:2. 

```{r import_data}
df <- import("data/gdp-quarterly-growth-eu.csv")
df %>% distinct(LOCATION)
df %>% head()
df <- df %>% select(LOCATION, TIME, Value) %>% spread(LOCATION, Value) %>% as_tibble()
```

```{r wrangle}
# Convert TIME variable to the date format using lubridate
df <- df %>% mutate(TIME = parse_date_time(TIME, orders = "%Y-%q"))

# Filter away what we don't use:
df <- df %>% filter(TIME > start_date, TIME < end_date) %>% select(all_of(countries),TIME)
```


## Stylised facts
Frequency for macroeconomic variables is low
Describe stylised facts, such as varying volatility
First lienar part, white noise
Second moment, many many lags are relevant. Conditional second order moments.

## HP filter

```{r HP}
# Create the volume index, necessary for the HP filter
#df_ix <- df %>% select(c(countries), TIME) %>% gather(key = "country", value = "change", c(countries))
to_pct <- function(x){
  x <- x/100
  return(x)
}

to_levels <- function(x){
  x <- cumprod(x + 1)
  return(x)
}

hpfilter1600 <- function(x){
  x <- mFilter::hpfilter(x, freq = 1600)
  return(x$cycle)
}

df_levels <- df %>% 
  mutate_at(countries, to_pct) %>%
  mutate_at(countries, to_levels)

# Plot the levels of the series
df_levels %>% gather(key = "country", value = "Index", c(countries)) %>% ggplot(aes(x = TIME, y = Index, color = country)) + geom_line()

# Apply HP filter and extract the cyclical component
df_levels <- df_levels %>% mutate_at(countries, hpfilter1600)

# Plot the remaining cyclical component:
df_levels %>% gather(key = "country", value = "Index", c(countries)) %>% ggplot(aes(x = TIME, y = Index, color = country)) + geom_line()

# convert to xts format
time <- df_levels$TIME
df_ts <- df_levels %>% select(-TIME)
df_ts <- xts(x=df_ts, order.by=time)
```

Think I should seasonally adjust it: poland and emu does the following:
seasonally adjusted dices in logarithms, referring to constant levels(?). Cyclical components are extracted using the HP filter ng appropriate parameter (i want quarterly)




# stationarity checks
## Descriptive statistics
* Jarque bera 
* ADF test


## ARIMA models (notes)
Ljung box. test jointly that veral autocorrelations of the returns are zero

Slide 29 chapter 1, stationarity assumption is unrealistic ecause of seasonality or cyclical components, remove those components in a first stage of the analysis. 

ARIMA models: Box Jenkins approach, slide 30. Not a big problem of portmanteau rejecting the null of white noise. No problem as the model selection was based on training, next we just care about how good the model is out of sample.

His code includes exmaple of GDP DEFLATOR and how to extract compinentsn out of it

Normality test using Jarque Bera

Unable to capture condtional second order moments
## GARCH 
DCC- MacGyver method???
Assumes that the conditional variance are deteminsitic min 44 lecture 3 at break to chapter 2.

### multivariate tationary process


```{r}
# Inspect current data:
df %>% gather(key = "country", value = "g", FRA, GBR, DEU, ESP) %>%
  ggplot(aes(x = TIME, y = g, color = country)) + geom_line()
```

```{r hp_filter}
# Deseasonalise using a HP filter to measure the cycle
# TODO: need a good theorectical back up for this
# Also, how does the function work?
# x <- mFilter::hpfilter(df$FRA, freq = 4)
```


```{r growth}

```

```{r volatiltiy clustering}

```


```{r rolling_corr}


```

# Method, garchfit

# Method, dynamic conditional correlation
Engle (2002) assumes the covariance matrix:

$ \mathbf{H}_t = \mathbf{\Sigma_t^{1/2}C_t\Sigma_t^{1/2}} $

Discuss parsimonity

Why not just use rolling correlation?
* no need to establish a window span
* loose observations at the start
* No shock persistencies
GO to cerqueira and martins (2009) for more of this discussion

```{r model}
# Create a DCC-GARCH specification object prior to fitting.
spec <- ugarchspec()
#spec # Info on what type of model we use inlcuding conditional distribution
multispec <- multispec(replicate(n_countries, spec)) # same as ugarchspec but multivariate setting
dcc_garch_spec <- dccspec(multispec)
```

```{r fit}
# dccfit
# Estimate univariate models first for robustness
multifit <- multifit(multispec, df_ts)
#multifit
# DCC model:
dccfit <- dccfit(dcc_garch_spec, df_ts)
#dccfit # replicates what was in the multifit (did we do the part step as a check?)
```

# Results

```{r extract_data}
# Extract the data to a format we can easily plot:
corrs <- rcor(dccfit)
corrs[,,dim(corrs)[3]] # Inspect last value

gen_dcc_data <- function(base_country){
 ix <- 1
 corr_df <- data.frame(corrs[base_country,base_country,])
 for (country in countries) {
   ix <- ix +1
   corr_df <- cbind(corr_df, corrs[base_country,country,])
   }
 names(corr_df) <- c("base", countries)
 corr_df <- corr_df %>% select(-base) # drop the initialising column
 # To access the date variable, convert to xts and then back to df
 corr_df <- fortify(as.xts(corr_df))
 # Gather to enable easy plotting
 corr_df <- corr_df %>% gather(key = "country", value = "correlation", c(FRA, GBR, DEU, ESP))
 return(corr_df)
}

plot_dcc_data <- function(df){
  p <- df %>% ggplot(aes(x = Index, y = correlation, color = country)) + 
    geom_line() + coord_cartesian(ylim = c(-0.1, 0.75))
  return(p)
}

# FRA
fra <- plot_dcc_data(gen_dcc_data(1)) + labs(title = "France")
# GBR
gbr <- plot_dcc_data(gen_dcc_data(2)) + labs(title = "United Kingdom")
# DEU
deu <- plot_dcc_data(gen_dcc_data(3)) + labs(title = "Germany")
# ESP
esp <- plot_dcc_data(gen_dcc_data(4)) + labs(title = "Spain")

(fra + gbr)/(deu + esp)


# Plot in xts style
#cor_FRA_ESP <- cbind(cor1[2,4,], cor1[2,4,])
#cor_FRA_ESP <- as.xts(esp_df)
#plot(cor_FRA_ESP)
```

## Table of DCC GARCH estimated results
* Conditional mean
* Conditional variance

## Compare to rolling correlation

```{r plot}
# plot(dccfit)

```

# Forecast where the conditional correlation is heading.

# Discussion of relevant periods
