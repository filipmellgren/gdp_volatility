---
title: "Euro area gdp synchronization"
author: "Filip Mellgren"
date: '2020-05-21'
output:
  html_document:
    df_print: kable
    code_folding: hide
  pdf_document:
    toc: true
bibliography: references.bib
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

```{r,include=FALSE}
library(rmgarch) # https://cran.r-project.org/web/packages/rmgarch/rmgarch.pdf
# More helpful slides on the above package: https://faculty.washington.edu/ezivot/econ589/DCCgarchPowerpoint.pdf
library(rio)
library(tidyverse)
library(ggplot2); theme_set(theme_minimal())
library(lubridate)
library(rmgarch)
library(xts)
library(patchwork)
library(zoo) # In tidyverse already?
library(mFilter)
library(forecast)
```

```{r parameters}
start_date <- as.POSIXct("1961-01-01")
end_date <- as.POSIXct("2019-12-01")
countries <-c("DEU", "FRA", "ITA", "ESP", "NLD", "BEL")
n_countries <- length(countries)
```

# TODOs
* Likelihood in second specification is lower than in the first meaning that it finds a local minimum. May need another start seed.
* Don't test for normality, but for whether residuals are t-distirbuted as I use this specification
* Test whether it makes sense to allow for the leverage effect (T-GARCH)
* Make plots more visual by adding indications of when the EMU was introduced etc.

# Introduction

For any common currency area, it is important that there is a certain degree of business cycle synchronization in order to make monetary policy effective [@frankel1998endogenity]. An example of a common currency area composed of countries having autonomous fiscal policy and tax policies is the Eurozone. It is therefore of interest to investigate to what extent the business cycles of countries within the Eurozone can be said to move together, and whether there has been a shift towards greater synchronisation after the introduction of the Euro in 1999.

To investigate the degree of business cycle synchronisation, I study the development of GDP across coutries over time using a dynamic conditional correlation (DCC) approach [@engle2002dynamic]. This model is a widely used benchmark model that is flexible without requiring too many parameters to be estimated.

# Data
The available data set contains information on seasonally adjusted percentage change in constant prices gross domestic product for 28 countries and a composite index of the Euro-area. The frequency at which the data is measured is quarterly and in the raw data, each value denotes the percentage change from one quarter to the next. This data set was downloaded from the OECD [@oecddata].

```{r import_data}
df <- import("data/gdp-quarterly-growth-eu2.csv")
df <- df %>% select(LOCATION, TIME, Value) %>% spread(LOCATION, Value) %>% as_tibble()
```

For most countries, the series start in the second quarter of 1960 and the analysis focuses on countries for which the time series have been available since this date. I restrict the number of countries further by focusing on the largest six remaining countries within the EMU, namely Germany, France, Italy, Spain, the Netherlands, and Belgium.


```{r wrangle}
# Convert TIME variable to the date format using lubridate
df <- df %>% mutate(TIME = parse_date_time(TIME, orders = "%Y-%q"))

# Filter away what we don't use, starting with countries without data:
df <- df %>% dplyr::filter(TIME > start_date, TIME < end_date) %>% select_if(~!any(is.na(.))) %>% select(all_of(countries),TIME)
```

```{r, results='asis'}
# Now, it looks like this:
#print(xtable::xtable(df %>% select(-TIME) %>% head()), comment=FALSE)
df %>% select(-TIME) %>% head()
```

## Stylised facts
By converting the series to indexed values, we immediately observe several key characteristics of the seasonally adjusted GDP data.

```{r HP}
# o) Eliminate remaining seasonality and other non-stationary components
# i) also, remove the trend here
# HP filter removes the trend, not the noise
# BP filter to smooth away the noise
# Create the volume index, necessary for the HP filter
#df_ix <- df %>% select(c(countries), TIME) %>% gather(key = "country", value = "change", c(countries))
from_pct <- function(x){
  x <- x/100
  return(x)
}

to_levels <- function(x){
  x <- cumprod(x + 1)
  return(x)
}

hpfilter1600 <- function(x){
  x <- mFilter::hpfilter(x, freq = 1600)
  return(x$cycle)
}

impute_na <- function(x){
  x <- ifelse(is.na(x), 0, x)
  return(x)
}

df_levels <- df %>% 
  mutate_at(countries, from_pct) %>%
  mutate_at(countries, impute_na) %>%
  mutate_at(countries, to_levels)

# Convert growth data into numeric values, i.e. don't use percentage form.
df <- df %>% 
  mutate_at(countries, from_pct)


# Plot the growth component of the unfiltered data:
qgrowth_plot <- df %>% gather(key = "country", value = "g", c(countries)) %>%
  ggplot(aes(x = TIME, y = g, color = country)) + geom_line() + labs(x = "Year")

# Plot the levels of the series
levels_plot <- df_levels %>% gather(key = "country", value = "Index", c(countries)) %>% ggplot(aes(x = TIME, y = Index, color = country)) + geom_line()

# Apply HP filter and extract the cyclical component
df_hp <- df_levels %>% mutate_at(countries, hpfilter1600)

# Plot the remaining cyclical component:
cyclical_plot <- df_hp %>% gather(key = "country", value = "Index", c(countries)) %>% ggplot(aes(x = TIME, y = Index, color = country)) + geom_line() + labs(x = "Year")
```

```{r plot_levels, fig.height = 2.5, fig.width = 7}
levels_plot + labs(x = "Year", title = "Indexed real GDP")
```

First, for all countries, the GDP series exhibits a clear upward trend. Second, for some countries, the trend seems to be varying over time. Another important feature is that volatility seems to vary. By looking at the quarter on quarter percentage growth series, it becomes clear that some periods are marked by significantly more severe swings than normal periods. In particular, it appears that  growth was the most volatile during the start of the series, entered a period of relative calmness, and was then severly disturbed again during the Great Recession of 2008 which is overall highly pronounced in the data. The period following the Great Recession was marked by a relatively large volatility during the Euro crisis. The series can thus safely be said to exhibit heteroskedasticity.

The fact that the series shows varying trends makes a direct comparison of the business cycles awry and it is therefore necessary to get rid of this trend. One alternative is to work with the growth data, another is to use a so called Hodrick-Prescott (HP) filter with frequency $\lambda = 1600$, which has been commonly employed on seasonally adjusted macroeconomic time series, $x_t$, in order to decompose them into $x_t = g_t + c_t$, where $g_t$ is a non stationary trend, and $c_t$ is the unobserved stationary cyclical residual. The idea would then be to work with the stationary component in this exercise. 

However, the HP filter is known to contain some flaws, for example, @cogley1995effects  states that the method is not suitable when the underlying process is believed to be integrated of order 1, in which case first differencing should be performed instead as the HP-filter risks creating business cycles that do not really exist in reality. Whether macroeconomic time series are drifting random walks or trend stationary is left out of the discussion and I choose to use the HP-filtered data because it seemed to give cleaner results.


In the next figure, the quarter to quarter growth rate is plotted above the cyclical component, $c_t$, from the HP-filter. As can be seen, the series look different making the choice an important one. For the growth data in particular, time varying volatility is a pronounced feature across countries.

```{r, fig.height = 5, fig.width = 7}
qgrowth_plot / cyclical_plot + theme(legend.position = "none")
```

```{r select_detrended_data}
# convert to xts format
time <- df_hp$TIME
df_ts <-df_hp %>% select(-TIME)
df_ts <- xts(x=df_ts, order.by=time)
```

# Method, dynamic conditional correlation

In this section, I further adapt the series by modelling the conditional means and variances using the GARCH(p, q) framework. By applying the GARCH framework, we are able to model the magnitude of the noise based on past innovations, thereby capturing the observation of time varying volatility. Because I am interested in correlations across countries, I need to estimate a time evolving covariance matrix across countries. This is done using the dynamic conditional correlation framework which allows for time varying correlations while being a relatively easy to estimate model compared to say a BEKK-GARCH model. Compared to the simplest method of computing rolling correlations, the DCC framework allows me to build a model, avoids the presence of shock persistencies, does not loose observations at the start, and does not require an arbitrary window parameter. It does, however, assume that the two parameters $\theta_1$, and $\theta_2$ are shared for all countries.

Formally, we need to model the first two conditional moments:

* $E[\epsilon_t \vert \mathcal{F}_{t-1}] = 0$
* $V(\epsilon_t \vert \mathcal{F}_{t-1}) = H_t$

Where the multivariate version of the strong GARCH is based on the following:

* $\epsilon_t = H_t^{1/2}z_t$

$\{ \mathbf{z}_t\}$ is a standardized vector of white noise, i.e. a process of iid variables with zero mean and a unit covariance matrix.

Thus, the conditional covariance matrix $H_t$ needs to be specified in order to pin down the process of $\epsilon_t$ using the DCC model by @engle2002dynamic.

* $\epsilon_t = H_t^{1/2}z_t$ 
* $H_t = D_t R_t D_t$, where the element at row $i$, column $i$ of matrix $D_t$ is $\sqrt{h_{ii, t}}$, else 0. Hence, $D_t$ is the matrix of individual conditional volatilities.
* $\mathbf{h}_t = w + \sum^q_{i = 1} \mathbf{A}_i \epsilon_{t-i}^2 + \sum^p_{j = 1}\mathbf{B}_j \mathbf{h}_{t-j}$, notice the similarity to an ARMA process.
* $w$ is an $m \times 1$ vector with positive coefficients
* $\mathbf{A}_i$ and $\mathbf{B}_j$ are $m \times m$ diagonal matrices with nonnegative coefficients.
* $R_t = (diag(Q_t))^{-1/2}Q_t(diag(Q_t))^{-1/2}$, where:
* $Q_t = (1 - \theta_1 - \theta_2) \bar{Q} + \theta_1 z_{t-1} z_{t-1}' + \theta_2 Q_{t-1}$ 
* Where $\hat{Q}$ is the unconditional correlation covariance matrix of the innovations $z_t$.
* $0 < \theta_1 + \theta_2 < 1$ With both $\theta_1$ and $\theta_2$ set to zero, we obtain the CCC model where the correlation matrix $R_t$ is assumed to be the constant $R = \bar{Q}$. Because $Q_t$ is modelled in a similar way as a GARCH(1,1), we don't want $\theta_1 + \theta_2$ to be too close to 1, as that would indicate a unit root in the conditional correlations.

What all this means is that we have a specification for a time varying specification of the correlation matrix $R_t$ that is $\mathcal{F}_{t-1}$-measurable, namely that we can estimate it using information available at time $t-1$. What we need to do is to find $p$ and $q$; the number of $\mathbf{B_j}$, and $\mathbf{A_i}$ matrices respectively such that $\mathbf{B_j}$, and $\mathbf{A_i}$ are positive semidefinite.

The fact that the matrices $\mathbf{A}_i$ and $\mathbf{B}_j$ are diagonal means we only model volatility as it propagates within countries. In theory, one could think of spillover effects and that volatility transmits from one country to another if the two are closely related and one country is hit by a shock that affects trade and flow of people. However, with diagonal matrices $\mathbf{A}_i$ and $\mathbf{B}_j$, it becomes possible to estimate the univariate time varying conditional volatilities in a first step using a standard or extended GARCH approach and the model does not explode with the number of countries.

The first estimation step starts by specifying the univariate conditional variances using GARCH methods for each country. For the univariate GARCH(p,q) model, there will be both an autoregressive, and a moving average component whose orders $p$ and $q$ can be pinned down with, for example, ACF and PACF plots of the squared error residuals. This procedure also needs to be done for the conditional mean which is assumed to follow an ARMA(p',q') model. Note that by first specifying univariate GARCH processes, we choose to not model the full variance covariance matrix, which includes volatility spillover effects. Instead, focus is to model conditional correlations over time.

From this step, the standardized residuals can be calculated which are used in a second step to calculate the unconditional correlation matrix $\bar{Q}$ from the data. Next, all the entries in the matrices $\mathbf{A_i}$, and $\mathbf{B_j}$ will need to be specified, along with the parameters $\theta_1$, and $\theta_2$. This is done by using the packages rugarch and rmgarch by @Ghalanos with built in quasi maximum likelihood routines. Ultimately, we obtain a conditional correlation matrix, $R_t$ and so the conditional covariance matrix $H_t$ which was our initial goal. We then check that the remaining residuals follows a white noise process to ensure that we were able to capture all the relevant information in our model.

## Estimation stage 


In this section, the goal is to arrive at a parsimonous specification of the conditional mean and variance. An initial consideration is whether to model all countries using the same model, or allow for flexible fits tailored to each country. Specific models would demand an analysis of each single series and be a more flexible approach than superimposing the same model for all series. However, I start with this approach to see whether it gives sufficient results. This is done by considering autocorrelation functions and partial autocorrealtion functions. The former looks at raw autocorrelations, and the latter after having partialled out previous lagged effects.

I choose to base the univariate ARMA-GARCH models on the German series, while using France as a confirmation of the soundness of the modelling choices. Hopefully, these two countries are representative enough to proide a good model.

---
TODO: It is possible that the normal distribution is unable to account for the large decrease in GDP during the Great Recession, which indicates a clear dent in the series. This is also specified in what follows.

In addition, I'm allowing for assymetric effects as volatility may reasonably be larger following negative growth (https://www.researchgate.net/profile/Kevin_Sheppard/publication/5213502_Asymmetric_Dynamics_in_the_Correlations_of_Global_Equity_and_Bond_Returns/links/0c96052df10e17001d000000/Asymmetric-Dynamics-in-the-Correlations-of-Global-Equity-and-Bond-Returns.pdf) negative correlation (instead of zero correlation) between the squared current innovation and the past innovations

Possible to test for a leverage effect.

Possible to plot a news impact curve
---


### Conditional mean

Starting with the conditional mean, I look at the ACF and PACF for Germany.


```{r mean_id_deu, fig.height = 3.5, fig.width = 3.5}
# ii) a priori identification of the orders p and q;
acf(data.frame(df_ts$DEU), main = "DEU") # data.frame because xts gave weird x-axis
pacf(data.frame(df_ts$DEU), main = "DEU")
```

By the ACF, we see that the series is dependent on past values up to 14 lags, or more than three years back in time and that values tend to be similar to those close in time, negatively related to values that are moderately distant, and unrelated to values stretching very far back. This pattern is consistent with the idea of a cycle.

By the PACF, we learn that the strongest relation is with the most recent value. If the last period was above (below) trend, the next period is likely to be above (below) as well. However, we do observe significant and negative lagged values up to order five.

Based on these graphs, a plausible model could be an AR process up to order 5. As is evident by the ACF, there is a cyclical pattern and by the PACF, this could be explained by several negative AR terms. However, because five is a rather long lag length I want to confirm it against another large euro area economy, France.


```{r, fig.height = 3.5, fig.width = 3.5}
acf(data.frame(df_ts$FRA), main = "FRA")
pacf(data.frame(df_ts$FRA), main = "FRA")
```
Overall the two look countries show similar patterns but the fifth lag is no longer significant.

Because we found reasons to believe the series exhibit heteroskedasticity in the stylized facts part, we need to consider adjusted confidence intervals that allow for conditional heteroskedasticity. Under a GARCH assumption the test statistic we want to consider becomes:

$$Q_m = T \hat{\rho}'_m \hat{\Sigma}^{-1}_{\hat{\rho}_m}\hat{\rho}_m$$

Where $\hat{\rho}_m$ denotes the estimate of the vector of the first $m$ sample autocorrelation and $\hat{\Sigma}^{-1}_{\hat{\rho}_m}$ is a variance covariance matrix. This test statistic is different from the Ljung Bo xstatistic, but becomes the Ljung box statistic when the variance covariance matrix is the identity matrix.

This test is a portmanteau test in which we specify the number of lags we want to consider. Under the null hypothesis, there is no autocorrelations in the data. In other words, all autoregressive terms have a zero coefficients up to the number of lags we specify.

```{r Audrinos_corr_box}
source("course_code.R")
corrbox8 <- corr.Box.test(df_ts$DEU, 8)
```

For Germany, the p-values associated with the test results at up to 8 levels is as follows:

* 8: `r round(corrbox8[1], 5)`

and we firmly reject the null of no autocorrelations. The plots above indicated that an autoregressive order of five is sufficient. However, France disputed the fifth order and to be more parsimonous, I start by only including autoregressive lags up to order four.

### Conditional variance

Having decided on a model for the conditional mean, I now turn to modelling the conditional variance. 


```{r, fig.height = 3.5, fig.width = 3.5}
acf(data.frame((df_ts$DEU)^2), main = "DEU") 
pacf(data.frame((df_ts$DEU)^2), main = "DEU")
```

Based on the ACF plot of the squared deviations, there is a persistent correlation with lagged values that could potentially be decaying exponentially, and then level out. The PACF suggests that the number of AR terms should either be limited to 1, 4 or 6. Hence, there is a good case to be made for starting with a GARCH(1,0) as the higher orders are barely significant. Nontheless, because the GARCH(1,1) model is commonly employed and the fact that the decay is not perfectly exponential, there is also a case to be made for including an MA term.


There is no apparent reason to allow the model to have a long memory of past volatility based on the graphs as the correlations fade to zero after relatively few lags. Economically, this means that turbulent growth eventually stabilise into more normal growth variability. 

for completeness, I provide p-value of the Lagrange multiplier test statistic including lagged effects up to order 8 to show that this lagged effect is present. At least under the assumption of a constant fourth order moment:

```{r}
arch_effect <- LM(df_ts$DEU, 8)
```

* Arch effect p-value: `r round(arch_effect,3)` (tested on Germany)


```{r mean_id_fra, fig.height = 3.5, fig.width = 3.5}
acf(data.frame((df_ts$FRA)^2), main = "FRA")
pacf(data.frame((df_ts$FRA)^2), main = "FRA")
```

For France, the overall picture is similar, but is decaying more exponentially, and indicates an order 2 AR term might be suitable.

Because the MA term is disputed, my initial thought was to go without it. However, that model did not converge and so I include an MA term for the model of the conditional variance.

Note that most of the plotted lagged autocorrelations are positive.

```{r}
manual_spec <- ugarchspec(mean.model = list(armaOrder = c(4,0)), variance.model = list(model = "sGARCH", garchOrder = c(1, 1)), distribution.model = "norm")

# If the student's t distribution is chosen:
manual_spec <- ugarchspec(mean.model = list(armaOrder = c(4,0)), variance.model = list(model = "sGARCH", garchOrder = c(1, 1)), distribution.model = "std")

# Note, the commented out specification was tested but lead to unit root GARCH dynamics (sum of GARCH components were close to 1).
```



### Choice of distribution

It is possible that the innovations cannot be described well by a normal distribution. For this reason, I estimated models of the conditional variance under the assumptions of normality and later that they follow a $t$-distribution. The $t$-distribution turned out to be better in the sense that the sum of the GARCH coefficients were further from unity, and so that is the model I will be working from.

---
### Assymetric volatilities

There are indications in the litterature that the real GDP growth rate 
Ho and Tsui (2003)
in which the negative real GDP shocks seem to have greater influence on future volatilities as compared to
positive shocks of the same magnitude (Asymmetric volatility of real GDP: Some evidence from Canada, Japan,
the United Kingdom and the United States)
---

## Estimated model
```{r model}
 # similar to ugarchspec but multivariate setting
multispecs <- multispec(replicate(n_countries, manual_spec))
dcc_specs<- dccspec(multispecs)
```

```{r estimate}
# iii) estimation of the parameters
# DCC model:
dcc.fit <- dccfit(dcc_specs, df_ts) 
show(dcc.fit)
```

Based on the output above, we have that $\hat{\theta}_1=$ `r round(dcc.fit@mfit$coef[["[Joint]dcca1"]], 3)`, and that $\hat{\theta}_2=$ `r round(dcc.fit@mfit$coef[["[Joint]dccb1"]], 3)`. Note that the sum is conveniently smaller than 1.

AR terms for the conditional mean are denoted as ".ar" in the output, MA terms as ".ma". For the conditional variance, AR terms are denoted as ".alpha", and MA terms as ".beta". ".mu" and ".omega" denote intercepts.

Adding up the GARCH related terms, we can see whether we have second order stationarity. 

```{r}
sum_garch_coef <- c()
for (c in countries){
  key_alpha <- glue::glue("[{c}].alpha1")
  key_beta <- glue::glue("[{c}].beta1")
  sum_garch_coef[c] <- dcc.fit@mfit$coef[[key_alpha]] + dcc.fit@mfit$coef[[key_beta]]
}
print(sum_garch_coef)
```

Based on the values above, second order stationarity is not obvious for most countries since the values are very close to 1. Values close to 1 means that the GARCH is an integrated process of order 1, i.e. a random walk. This is problematic, but also typical, as it indicates the conditional variance is exploding (not reverting to back to its mean level). Potential remedies include adding more AR/MA terms to get rid of lags effect that I failed to account for, or estimating the model using a distribution with (even) fatter tails. This specification arose from a specification allowing for a student's t-distribution rather than a normal distribution, which slightly improved the results.

At the risk of overfitting the data, I construct the McLeod, Li test for remaining conditional heteroskedasticity in the residuals using the course code. This is done by passing the standardised residuals to the function used above which were used to confirm the existence of heteroskedasticity for remaining lags  of orders 2, 4, and 8.

```{r standardise_residuals}
# Here, I standardise the residuals by dividing by the estimated standard deviation
dcc.res <- residuals(dcc.fit)
dcc.sig <- sigma(dcc.fit)
dcc.std_res <- dcc.res/dcc.sig
```

```{r remaining_garch, results="asis"}
LM2 <- function(dataseries){
  test_p_value <- LM(dataseries, 2)
  return(test_p_value)
}

LM4 <- function(dataseries){
  test_p_value <- LM(dataseries, 4)
  return(test_p_value)
}

LM8 <- function(dataseries){
  test_p_value <- LM(dataseries, 8)
  return(test_p_value)
}

rem_garch2 <- sapply(dcc.std_res, LM2)
rem_garch4 <- sapply(dcc.std_res, LM4)
rem_garch8 <- sapply(dcc.std_res, LM8)


LM_test <- cbind(rem_garch2, rem_garch4, rem_garch8) %>% data.frame()

LM_test <- LM_test %>% mutate(rem_garch2 = round(rem_garch2, 3),
                   rem_garch4 = round(rem_garch4, 3),
                   rem_garch8 = round(rem_garch8, 3)) %>%
  rename("Test at level 2" = rem_garch2,
         "Test at level 4" = rem_garch4,
         "Test at level 8" = rem_garch8)


print(xtable::xtable(cbind(countries, LM_test)), comment=FALSE)
cbind(countries, LM_test)
```

In the table above, P-values are plotted for the McLeod Li test at various levels of lags. The null is again that there is no remaining autoregressive GARCH effects present in the data. We find that most countries are seemingly well defined; only Spain and Italy seem to exhibit autocorrelated squared residuals. See the two ACF and PACF plots for Italy:


```{r fig.height = 3.5, fig.width = 3.5}
acf(data.frame((df_ts$ITA)^2), main = "ITA")
pacf(data.frame((df_ts$DEU)^2), main = "ITA")
```

Next, I also check whether the conditional mean was well modelled by considering the adjusted Portmanteu test statistic on the residuals:

```{r results="asis"}
corrbox2 <- function(dataseries){
  test_p_value <- corr.Box.test(dataseries, 2)
  return(test_p_value)
}

corrbox4 <- function(dataseries){
  test_p_value <- corr.Box.test(dataseries, 4)
  return(test_p_value)
}

corrbox8 <- function(dataseries){
  test_p_value <- corr.Box.test(dataseries, 8)
  return(test_p_value)
}

rem_arma2 <- sapply(dcc.std_res, corrbox2)
rem_arma4 <- sapply(dcc.std_res, corrbox4)
rem_arma8 <- sapply(dcc.std_res, corrbox8)

corrbox_test <- cbind(rem_arma2, rem_arma4, rem_arma8) %>% data.frame()

corrbox_test <- corrbox_test %>% mutate(rem_arma2 = round(rem_arma2, 4),
                   rem_arma4 = round(rem_arma4, 4),
                   rem_arma8 = round(rem_arma8, 4)) %>%
  rename("Test at level 2" = rem_arma2,
         "Test at level 4" = rem_arma4,
         "Test at level 8" = rem_arma8)


print(xtable::xtable(cbind(countries, corrbox_test)), comment=FALSE)
cbind(countries, corrbox_test)
```

This test gives a very strong signal that we need to add an ARMA component. Based on ACF and PACF plots for the Netherlands and Belgium, it is hard to judge what is required and results  for Italy and Spain will be influenced by the need for an extra GARCH component. I therefore decide to add both an extra AR term, and an MA term for all the univariate specifications. I also add an additional GARCH component in the form of an AR term to see if I can accomodate the remaining autocorrealtions for the conditional variance.

The model under consideration is now thus:

* ARMA(5,1)
* GARCH(2,1)

For all the univariate series. Otherwise, the DCC specification remains similar.

## Reestimated model

```{r}
# New model
new_spec <- ugarchspec(mean.model = list(armaOrder = c(5,1)), variance.model = list(model = "sGARCH", garchOrder = c(2, 1)), distribution.model = "std")

new.ugarch_spec_list <- replicate(n_countries, new_spec)

new.multispecs <- multispec(new.ugarch_spec_list) # similar to ugarchspec but multivariate setting
new.dcc_specs<- dccspec(new.multispecs)

new.dcc.fit <- dccfit(new.dcc_specs, df_ts) 
show(new.dcc.fit)
```
```{r}
sum_garch_coef <- c()
for (c in countries){
  key_alpha <- glue::glue("[{c}].alpha1")
  key_alpha2 <- glue::glue("[{c}].alpha2")
  key_beta <- glue::glue("[{c}].beta1")
  sum_garch_coef[c] <- new.dcc.fit@mfit$coef[[key_alpha]] + new.dcc.fit@mfit$coef[[key_alpha2]] + new.dcc.fit@mfit$coef[[key_beta]]
}
print(sum_garch_coef)
```

Based on the results above, we see that we still struggle with a unit root in the conditional variance, albeit the problem is somewhat alleviated. In addition, the AIC actually increases, which means we are starting to overspecify the processes. I go on to analyse the residuals of this last model, if they are not much better than for the first model, the final DCC results of the time evolving correlations will be based on the first model.

## Model diagnostics stationarity checks

```{r standardise_residuals2}
# Here, I standardise the residuals by dividing by the estimated standard deviation
dcc.res <- residuals(new.dcc.fit)
dcc.sig <- sigma(new.dcc.fit)
dcc.std_res <- dcc.res/dcc.sig

```


### Remaining serial correlation, Test model for mean
Now, I test whether the estimated GARCH model leads to serially uncorrelated error terms. Ideally, there should not remain any conditional volatility in the seires. The first step towards checking this is to standardise the residuals by dividing them with the estimated conditional standard deviation. Note that this value will in general be unique for each country-time pair because of the properties of the DCC model. The corrected Ljung-Box Q(8)-statistic, is formed which gives a test of the null hypothesis that the lags 1 through 8 are not significantly different from 0, i.e. that the residuals from the conditional mean model are unpredictable.

The choice of 8 is somewhat arbitrary but corresponds to two years of data, which seems long enough to accomodate any lagged effects, and short enough to not dilute the power of the test with lags that are so far away that they likely have no effect on the residuals. Ideally then, this test is not significant for any of the countries if the model suits the data well enough.


```{r}
# iv) validation based on residual analysis
rem_arma2 <- sapply(dcc.res, corrbox2)
rem_arma4 <- sapply(dcc.res, corrbox4)
rem_arma8 <- sapply(dcc.res, corrbox8)

corrbox_test <- cbind(rem_arma2, rem_arma4, rem_arma8) %>% data.frame()

corrbox_test <- corrbox_test %>% mutate(rem_arma2 = round(rem_arma2, 4),
                   rem_arma4 = round(rem_arma4, 4),
                   rem_arma8 = round(rem_arma8, 4)) %>%
  rename("Test at level 2" = rem_arma2,
         "Test at level 4" = rem_arma4,
         "Test at level 8" = rem_arma8)

cbind(countries, corrbox_test)
```

Again, I find that the conditional mean is highly autocorrelated. remaining autocorrelations for most countries when testing for autocorrelations up to order 8. 

I provide a plot of the residuals for the German series to check what the mean residuals look like:

```{r}
plot(dcc.std_res$DEU)
```
It is hard to say why there remains serial correlation by looking at the plot above, also the acf and pacf plots give little indication other than that very distant lags seem relevant, which is hard to explain and I choose to ignore the issue.

```{r fig.height = 3.5, fig.width = 3.5}
acf(data.frame((dcc.std_res$DEU)), main = "DEU")
pacf(data.frame((dcc.std_res$DEU)), main = "DEU")
```

Next, we want to check whether the residuals are normally distributed. This is tested using Jarque Bera test for normality.

```{r}
jb_test <- sapply(dcc.std_res, tseries::jarque.bera.test)

jb_test_p <- cbind(jb_test) %>% data.frame()
jb_test_p
```

As is often the case, the case rejects normality of the residuals. Below is a QQ plot of the German residuals which shows that the tails are probably fatter than that of a normal:

```{r condmean_normality}
# Jarque Bera test for normality coupled with QQ plot
qqnorm(dcc.std_res$DEU)
qqline(dcc.std_res$DEU)
```


### Remaining GARCH effects

Next, I want to test whether there are remaining GARCH effects, or if the model of the conditional variance is correctly specified. The procedure is similar to what we did before when we tested for remaining autocorrealtion in the second moment.

```{r cond_variance_LB}
rem_garch2 <- sapply(dcc.std_res, LM2)
rem_garch4 <- sapply(dcc.std_res, LM4)
rem_garch8 <- sapply(dcc.std_res, LM8)

LM_test <- cbind(rem_garch2, rem_garch4, rem_garch8) %>% data.frame()

LM_test <- LM_test %>% mutate(rem_garch2 = round(rem_garch2, 3),
                   rem_garch4 = round(rem_garch4, 3),
                   rem_garch8 = round(rem_garch8, 3)) %>%
  rename("Test at level 2" = rem_garch2,
         "Test at level 4" = rem_garch4,
         "Test at level 8" = rem_garch8)

cbind(countries, LM_test)
```


Based on the table above, the model seems capable of reducing the noise component to become unpredictable for some series, but fails drastically for Spain. 
To give a sense of the series, I provide a plot of the German residuals below:

```{r condvar_normality}
plot((dcc.std_res^2)$DEU)
```


```{r}
# Likelihood comparison slide 27 chapter 3
```




# Results

---
## Conditional mean for some country

## Conditional variance for some country
---

## Conditional correlations
Plotted conditional correlations are for the later model, which was somewhat better at getting rid of the unit root. 

```{r extract_data}
# Extract the data to a format we can easily plot:
corrs <- rcor(new.dcc.fit)

gen_dcc_data <- function(base_country){
  # Function generates a data frame of correlations of one country with all others
 ix <- 1
 corr_df <- data.frame(corrs[base_country,base_country,])
 for (country in countries) {
   ix <- ix +1
   corr_df <- cbind(corr_df, corrs[base_country,country,])
   }
 names(corr_df) <- c("base", countries)
 corr_df <- corr_df %>% select(-base) # drop the initialising column
 # To access the date variable, convert to xts and then back to df
 corr_df <- fortify(as.xts(corr_df))
 # Gather to enable easy plotting
 corr_df <- corr_df %>% gather(key = "country", value = "correlation", countries)
 return(corr_df)
}

# Plot data
plot_dcc_data <- function(country_ix){
  df <- gen_dcc_data(country_ix)
  p <- df %>% ggplot(aes(x = Index, y = correlation, color = country)) + 
    geom_line() + coord_cartesian(ylim = c(-0.1, 0.6)) +
    labs(subtitle = countries[country_ix], x = "Year", y = "Correlation") +
    theme(legend.position = "none")
  return(p)
}

corr_plots <- lapply(1:length(countries), plot_dcc_data)
```


```{r, fig.height = 10, fig.width = 7}
corr_plots[[1]] / corr_plots[[2]] / corr_plots[[3]] + theme(legend.position = "bottom")
```

```{r, fig.height = 10, fig.width = 7}
corr_plots[[4]] / corr_plots[[5]] +corr_plots[[6]] + theme(legend.position = "bottom")
```


# Discussion

The estimates for the conditional correlation show periods of varying correlation between countries. It seems to have increased during the Great Recession, and then gradually reverting. There is no clear evidence that the correlation has been increasing over the considered time horizon based on this analysis. If anything, there is a small indication that correlation increased during the 90's and 00's. However, since the residuals were not white noise and there seems to have been a unit root present in the conditional variance, it is unclear whether any strong conclusions can be made based on this.


# References
