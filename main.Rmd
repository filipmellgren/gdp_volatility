---
title: "GDP Volatility"
author: "Filip Mellgren"
date: '2020-05-09'
output:
  html_document:
    df_print: kable
  pdf_document:
    toc: true
bibliography: references.bib
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
```

```{r,include=FALSE}
library(rmgarch) # https://cran.r-project.org/web/packages/rmgarch/rmgarch.pdf
# More helpful slides on the above package: https://faculty.washington.edu/ezivot/econ589/DCCgarchPowerpoint.pdf
library(rio)
library(tidyverse)
library(ggplot2); theme_set(theme_minimal())
library(lubridate)
library(rmgarch)
library(xts)
library(patchwork)
library(zoo) # In tidyverse already?
library(mFilter)
library(forecast)
```

```{r parameters}
start_date <- as.POSIXct("1961-01-01")
end_date <- as.POSIXct("2019-12-01")
countries <- c("FRA", "ITA", "DEU", "ESP", "AUT")
n_countries <- length(countries)
```

egarch for leverage effect
assymetric dcc for assymetric postiive and begative effects: https://www.researchgate.net/profile/Kevin_Sheppard/publication/5213502_Asymmetric_Dynamics_in_the_Correlations_of_Global_Equity_and_Bond_Returns/links/0c96052df10e17001d000000/Asymmetric-Dynamics-in-the-Correlations-of-Global-Equity-and-Bond-Returns.pdf



# Introduction

For any common currency area, it is important that there is a certain degree of business cycle synchronization in order to make monetary policy effective [@frankel1998endogenity]. An example of a common currency area composed of countries having autonomous fiscal policy and tax policies is the Eurozone. It is therefore of interest to investigate to what extent the business cycles of countries within the Eurozone can be said to move together, and whether there has been a shift towards greater synchronisation after the introduction of the Euro in 1999.

To investigate the degree of business cycle synchronisation, I study the development of GDP across coutries over time using a dynamic conditional correlation (DCC) approach [@engle2002dynamic]. This model is a widely used benchmark model that is flexible without requiring too many parameters to be estimated.

# Data
The available data set contains information on seasonally adjusted percentage change in constant prices gross domestic product for TODO countries. The frequency at which the data is measured is quarterly and in the raw data, each value denotes the percentage change from one quarter to the next. This data set was downloaded from the OECD [@oecddata].

```{r import_data}
df <- import("data/gdp-quarterly-growth-eu2.csv")
df <- df %>% select(LOCATION, TIME, Value) %>% spread(LOCATION, Value) %>% as_tibble()
```

For most countries, the series start in the second quarter of 1960 and the analysis focuses on countries for which the time series have been available since this date, limitting the analysis to EMU countries in western Europe.


```{r wrangle}
# Convert TIME variable to the date format using lubridate
df <- df %>% mutate(TIME = parse_date_time(TIME, orders = "%Y-%q"))

# Filter away what we don't use, starting with countries without data:
df <- df %>% filter(TIME > start_date, TIME < end_date) %>% select_if(~!any(is.na(.))) %>% select(all_of(countries),TIME)
```
```{r}
# Now, it looks like this:
df %>% head()
```


## Stylised facts
By converting the series to indexed values, we immediately observe several key characteristics of the seasonally adjusted GDP data.

```{r HP}
# o) Eliminate remaining seasonality and other non-stationary components
# i) also, remove the trend here

# HP filter removes the trend, not the noise
# BP filter to smooth away the noise
# Create the volume index, necessary for the HP filter
#df_ix <- df %>% select(c(countries), TIME) %>% gather(key = "country", value = "change", c(countries))
from_pct <- function(x){
  x <- x/100
  return(x)
}

to_levels <- function(x){
  x <- cumprod(x + 1)
  return(x)
}

hpfilter1600 <- function(x){
  x <- mFilter::hpfilter(x, freq = 1600)
  return(x$cycle)
}

impute_na <- function(x){
  x <- ifelse(is.na(x), 0, x)
  return(x)
}

df_levels <- df %>% 
  mutate_at(countries, from_pct) %>%
  mutate_at(countries, impute_na) %>%
  mutate_at(countries, to_levels)

# Convert growth data into numeric values, i.e. don't use percentage form.
df <- df %>% 
  mutate_at(countries, from_pct)


# Plot the growth component of the unfiltered data:
qgrowth_plot <- df %>% gather(key = "country", value = "g", c(countries)) %>%
  ggplot(aes(x = TIME, y = g, color = country)) + geom_line()

# Plot the levels of the series
levels_plot <- df_levels %>% gather(key = "country", value = "Index", c(countries)) %>% ggplot(aes(x = TIME, y = Index, color = country)) + geom_line()

# Apply HP filter and extract the cyclical component
df_hp <- df_levels %>% mutate_at(countries, hpfilter1600)

# Plot the remaining cyclical component:
cyclical_plot <- df_hp %>% gather(key = "country", value = "Index", c(countries)) %>% ggplot(aes(x = TIME, y = Index, color = country)) + geom_line()
```

```{r plot_levels, fig.height = 2.5, fig.width = 7}
levels_plot
```

First, for all countries, the GDP series exhibits a clear upward trend. Second, for some countries, the trend seems to be varying over time. Another important feature is that volatility seems to vary. By looking at the quarter on quarter percentage growth series, it becomes clear that some periods are marked by significantly more severe swings than normal periods. In particular, it appears that  growth was the most volatile during the start of the series, entered a period of relative calmness, and was then severly disturbed again during the Great Recession of 2008 which is overall highly pronounced in the data. The period following the Great Recession was marked by a relatively large volatility during the Euro crisis. The series can thus safely be said to exhibit heteroskedasticity.

The fact that the series shows varying trends makes a direct comparison of the business cycles awry and it is therefore necessary to get rid of this trend. One alternative is to work with the growth data, another is to use a so called Hodrick-Prescott (HP) filter with frequency $\lambda = 1600$, which has been commonly employed on seasonally adjusted macroeconomic time series, $x_t$, in order to decompose them into $x_t = g_t + c_t$, where $g_t$ is a non stationary trend, and $c_t$ is the unobserved stationary cyclical residual. The idea would then be to work with the stationary component in this exercise. 

However, the HP filter is known to contain some flaws, for example, @cogley1995effects  states that the method is not suitable when the underlying process is believed to be integrated of order 1, in which case first differencing should be performed instead as the HP-filter risks creating business cycles that do not really exist in reality. Whether macroeconomic time series are drifting random walks or trend stationary is left out of the discussion and I choose to use the growth data.


In the next figure, the quarter to quarter growth rate is plotted above the cyclical component, $c_t$, from the HP-filter. As can be seen, the series look different making the choice an important one.

```{r, fig.height = 5, fig.width = 7}
qgrowth_plot / cyclical_plot + theme(legend.position = "none")
```

```{r select_detrended_data}
# convert to xts format
time <- df_hp$TIME
df_ts <- df_hp %>% select(-TIME)
df_ts <- xts(x=df_ts, order.by=time)
```

# Method, dynamic conditional correlation

In this section, I further adapt the series by modelling the conditional means and variances using the GARCH(p, q) framework. By applying the GARCH framework, we are able to model the magnitude of the noise based on past innovations, thereby capturing the observation of time varying volatility. Because I am interested in correlations across countries, I need to estimate a time evolving covariance matrix across countries. This is done using the dynamic conditional correlation framework which allows for time varying correlations while being a relatively easy to estimate model compared to say a BEKK-GARCH model. Compared to the simplest method of computing rolling correlations, the DCC framework allows me to build a model, avoids the presence of shock persistencies, does not loose observations at the start, and does not require an arbitrary window parameter.

Formally, we need to model the first two conditional moments:

* $E[\epsilon_t \vert \mathcal{F}_{t-1}] = 0$
* $V(\epsilon_t \vert \mathcal{F}_{t-1}) = H_t$

Where the multivariate version of the strong GARCH is based on the following:

* $\epsilon_t = H_t^{1/2}z_t$

$\{ \mathbf{z}_t\}$ is a standardized vector of white noise, i.e. a process of iid variables with zero mean and a unit covariance matrix.

Thus, the conditional covariance matrix $H_t$ needs to be specified in order to pin down the process of $\epsilon_t$ using the DCC model by @engle2002dynamic.

* $\epsilon_t = H_t^{1/2}z_t$ 
* $H_t = D_t R_t D_t$, where the element at row $i$, column $i$ of matrix $D_t$ is $\sqrt{h_{ii, t}}$, else 0. Hence, $D_t$ is the matrix of individual conditional volatilities.
* $\mathbf{h}_t = w + \sum^q_{i = 1} \mathbf{A}_i \epsilon_{t-i}^2 + \sum^p_{j = 1}\mathbf{B}_j \mathbf{h}_{t-j}$, notice the similarity to an ARMA process.
* $w$ is an $m \times 1$ vector with positive coefficients
* $\mathbf{A}_i$ and $\mathbf{B}_j$ are $m \times m$ diagonal matrices with nonnegative coefficients.
* $R_t = (diag(Q_t))^{-1/2}Q_t(diag(Q_t))^{-1/2}$, where:
* $Q_t = (1 - \theta_1 - \theta_2) \bar{Q} + \theta_1 z_{t-1} z_{t-1}' + \theta_2 Q_{t-1}$ 
* Where $\hat{Q}$ is the unconditional correlation covariance matrix of the innovations $z_t$.
* $0 < \theta_1 + \theta_2 < 1$ With both $\theta_1$ and $\theta_2$ set to zero, we obtain the CCC model where the correlation matrix $R_t$ is assumed to be the constant $R = \bar{Q}$.

What all this means is that we have a specification for a time varying specification of the correlation matrix $R_t$ that is $\mathcal{F}_{t-1}$-measurable, namely that we can estimate it using information available at time $t-1$. What we need to do is to find $p$ and $q$; the number of $\mathbf{B_j}$, and $\mathbf{A_i}$ matrices respectively such that $\mathbf{B_j}$, and $\mathbf{A_i}$ are positive semidefinite.

The fact that the matrices $\mathbf{A}_i$ and $\mathbf{B}_j$ are diagonal means we only model volatility as it propagates within countries. In theory, one could think of spillover effects and that volatility transmits from one country to another if the two are closely related and one country is hit by a shock that affects trade and flow of people. However, with diagonal matrices $\mathbf{A}_i$ and $\mathbf{B}_j$, it becomes possible to estimate the univariate time varying conditional volatilities in a first step using a standard or extended GARCH approach and the model does not explode with the number of countries.

The first estimation step starts by specifying the univariate conditional variances using GARCH methods for each country. For the univariate GARCH(p,q) model, there will be both an autoregressive, and a moving average component whose orders $p$ and $q$ can be pinned down with, for example, ACF and PACF plots of the squared error residuals. This procedure also needs to be done for the conditional mean which is assumed to follow an ARMA(p',q') model. Note that by first specifying univariate GARCH processes, we choose to not model the full variance covariance matrix, which includes volatility spillover effects. Instead, focus is to model conditional correlations over time.

From this step, the standardized residuals can be calculated which are used in a second step to calculate the unconditional correlation matrix $\bar{Q}$ from the data. Next, all the entries in the matrices $\mathbf{A_i}$, and $\mathbf{B_j}$ will need to be specified, along with the parameters $\theta_1$, and $\theta_2$. This is done by using the packages rugarch and rmgarch by @Ghalanos with built in quasi maximum likelihood routines. Ultimately, we obtain a conditional correlation matrix, $R_t$ and so the conditional covariance matrix $H_t$ which was our initial goal. We then check that the remaining residuals follows a white noise process to ensure that we were able to capture all the relevant information in our model.


---
$h_t$ matrix of the indivudal conditional vairances. Can be written as the formula fof a garch. With diagonal maytrices A and B, we get no spillover effects. A and B must be posirtive semidefinite. With diagonality imposed, it is the same as second order stationarity, non negatvity of the coefficients. Strict positivity of the constant coefficient for the different models. 
$D_t$ diagonal matrix with indoivdual conditional volatilties
$\epsilon_t^2$ is a vector of the squared past returns for the m assets
$H_t = D_t R D_t$ COnditional covariance

# Assumes that the conditional variance are deteminsitic min 44 lecture 3 at break to chapter 2.
# Second order stationarity slide 4 ch 4.
---

## Estimation stage 

---
(identification of orders p and q, step 2 slide 30 chapter 1)
---

In this section, the goal is to arrive at a parsimonous specification of the conditional mean and variance. An initial consideration is whether to model all countries using the same model, or allow for flexible fits tailored to each country. While allowing for specific models is somewhat less parsimonous than imposing the same structure, I choose to allow for specific models because the series are different and may thus behave differently. This is implemented by automatic modelling using the the function auto.arima from the R package forecast by @hyndmanforecast. This can be used for both the conditional mean and variance, and the model for the variance is obtained by first squaring the series. However, I also provide classic ACF and PACF plots for Germany to characterise the data.

The alternative to modelling each country individually would be to limit the analysis to focus on a single country, and then apply the best model on all other countries. This path was pursued but did was unsuccesful because the series display large differences in behavior. 

It is possible that the normal distribution is unable to account for the large decrease in GDP during the Great Recession, which indicates a clear dent in the series. This is also specified in what follows.

---
In addition, I'm allowing for assymetric effects as volatility may reasonably be larger following negative growth (https://www.researchgate.net/profile/Kevin_Sheppard/publication/5213502_Asymmetric_Dynamics_in_the_Correlations_of_Global_Equity_and_Bond_Returns/links/0c96052df10e17001d000000/Asymmetric-Dynamics-in-the-Correlations-of-Global-Equity-and-Bond-Returns.pdf) negative correlation (instead of zero correlation) between the squared current innovation and the past innovations

Possible to test for a leverage effect.

Possible to plot a news impact curve
---

### Conditional mean
Starting with the conditional mean, I look at the ACF and PACF for Germany.


```{r mean_id_deu, fig.height = 3.5, fig.width = 3.5}
# ii) a priori identification of the orders p and q;
acf(data.frame(df_ts$DEU), main = "DEU") # data.frame because xts gave weird x-axis
pacf(data.frame(df_ts$DEU), main = "DEU")
```


By the ACF, we see that the series is dependent on past values up to 14 lags, or more than three years back in time and that values tend to be similar to those close in time, negatively related to values that are moderately distant, and unrelated to values stretching very far back. This pattern is consistent with the idea of a cycle.

By the PACF, we learn that the strongest relation is with the most recent value. If the last period was above (below) trend, the next period is likely to be above (below) as well. However, we do observe significant and negative lagged values up to order five.

Based on these graphs, a plausible model could be an AR process up to order 5. As is evident by the ACF, there is a cyclical pattern and by the PACF, this could be explained by several negative AR terms. However, because five is a rather long lag length I want to confirm it against another large euro area economy, France.

```{r mean_id_fra, fig.height = 3.5, fig.width = 3.5}
acf(data.frame(df_ts$FRA), main = "FRA")
pacf(data.frame(df_ts$FRA), main = "FRA")
```

The pattern looks similar and there is a significant lag of order six. However, the first lags are weak and the fifth is not pronounced and I therefore choose a lag of order four, which includes the same quarter last year, and one lag for each other quarter which seems reasonable.

### Conditional variance
Next, I consider a model of the variance by squaring the series.

```{r, fig.height = 3.5, fig.width = 3.5}
acf(data.frame((df_ts$DEU)^2), main = "DEU") 
pacf(data.frame((df_ts$DEU)^2), main = "DEU")
```
Test the GARCH(1,1), 

Based on the ACF plot of the squared deviations, there is a persistent correlation with lagged values that could potentially be decaying exponentially, and then level out. The PACF suggests that the number of AR terms should be limited to 1. Hence, there is a good case to be made for a GARCH(1,0). Nontheless, because the GARCH(1,1) model is commonly employed and the fact that the decay is not perfectly exponential, I choose to include an MA term and use this as my only specification for the conditional variance. 

There is no apparent reason to allow the model to have a long memory of past volatility based on the graphs as the correlations fade to zero after relatively few lags. Economically, this means that turbulent growth eventually stabilise into more normal growth variability.

```{r auto_models}
# Create univariate GARCH specification object prior to fitting.
spec_init <- ugarchspec() # Need this to initiate the list
ugarch_spec_list <- c(spec_init)
ix <- 0
for (c in countries){
  ix <- ix + 1
  series <- data.frame(df_ts) %>% select(c)
  arma_model <- auto.arima(series, d = 0,  allowdrift = FALSE, allowmean = FALSE)
  ar_p <- arma_model$arma[1]
  ma_q <- arma_model$arma[2]
  garch_model <- auto.arima(series^2, d = 0, allowdrift = FALSE, allowmean = FALSE)
  var_ar_p <- garch_model$arma[1]
  var_ma_p <- garch_model$arma[2]
  spec <- ugarchspec(mean.model = list(armaOrder = c(ar_p,ma_q)), variance.model = list(model = "sGARCH", garchOrder = c(var_ar_p, var_ma_p)), distribution.model = "snorm")
  ugarch_spec_list[ix] <- spec
}

```

### Choice of distribution
It is possible that the innovations cannot be described well by a normal distribution. For this reason, I estimate models of the conditional variance under the assumptions of normality and that they follow a $t$-distribution.

### Assymetric volatilities

There are indications in the litterature that the real GDP growth rate 
Ho and Tsui (2003)
in which the negative real GDP shocks seem to have greater influence on future volatilities as compared to
positive shocks of the same magnitude (Asymmetric volatility of real GDP: Some evidence from Canada, Japan,
the United Kingdom and the United States)

## Estimated model
```{r model}
multispecs <- multispec(ugarch_spec_list) # similar to ugarchspec but multivariate setting
dcc_specs<- dccspec(multispecs)
```

```{r estimate}
# iii) estimation of the parameters
# DCC model:
dcc.fit <- dccfit(dcc_specs, df_ts) 
dcc.fit
```
The joint params must be the two extra DCC parameters, close to 1. These params are assumed to be the same for all assets is a rather strong assumption. For this reason, we can find sub groups with different parameters.

Comment on stationarity, slide 17 ch 2.

## Model diagnostics stationarity checks

```{r standardise_residuals}
# Here, I standardise the residuals by dividing by the estimated standard deviation
dcc.res <- residuals(dcc.fit)
dcc.sig <- sigma(dcc.fit)
dcc.std_res <- dcc.res/dcc.sig
```


### Remaining serial correlation, Test model for mean
Now, I test whether the estimated GARCH model leads to serially uncorrelated error terms. Ideally, there should not remain any conditional volatility in the seires. The first step towards checking this is to standardise the residuals by dividing them with the estimated conditional standard deviation. Note that this value will in general be unique for each country-time pair because of the properties of the DCC model. The Ljung-Box Q(8)-statistic, is formed which gives a test of the null hypothesis that the lags 1 through 8 are not significantly different from 0, i.e. that the residuals from the AR model follow a white noise process. The choice of 8 is somewhat arbitrary but corresponds to two years of data, which seems long enough to accomodate any lagged effects, and short enough to not dilute the power of the test with lags that are so far away that they likely have no effect on the residuals. Ideally then, this test is not significant for any of the countries if the model suits the data well enough.




```{r}
# iv) validation based on residual analysis
# Ljung Box test
LB_test_p <- c()
for (c in countries) {
  test_residuals <- data.frame(dcc.std_res) %>% select(c)
  p_value <- Box.test(test_residuals, type = "Ljung", lag = 8)$p.value
  LB_test_p <- c(LB_test_p, p_value)
}
LB_test_table <- tibble(countries, LB_test_p)
plot(dcc.std_res$ESP)


# Jarque Bera test for normality coupled with QQ plot
qqnorm(dcc.std_res$DEU)
qqline(dcc.std_res$DEU)
```
For the conditional mean, I find that...

```{r, results = "asis"}
print(xtable::xtable(LB_test_table), comment=FALSE)
```


### Remaining GARCH efects

---
(Engle p137). Look at acf and pacf of the squared residuals.
ARCH models are estimated simultanously by ML. Test for remaining Garch errors using McLeod Li
Theorem of second order stationarity of the GARCH 11 process slide 19 ch 2.
---

Next, I want to test whether there are remaining GARCH effects, or if the model of the conditional variance is correctly specified. The procedure is similar to when the residuals of the model of the conditional mean was tested with the main difference that it is now the squared standardized series that is being tested. Hence, a Ljung-Box Q(8) statistic is formed for each country.

```{r cond_variance_LB}
LB_test_var_p <- c()
for (c in countries) {
  test_residuals <- (data.frame(dcc.std_res) %>% select(c))^2
  p_value <- Box.test(test_residuals, type = "Ljung", lag = 8)$p.value
  LB_test_var_p <- c(LB_test_var_p, p_value)
}
LB_test_table <- tibble(countries, LB_test_var_p)


plot((dcc.std_res^2)$ESP)
```


```{r}
# Likelihood comparison slide 27 chapter 3
```



```{r model_selection}
# v) select a model 
```


```{r selection}
# From the better model, we extract the squared residuals for Germany
#german_shock <- (residuals((dccfit_ar4_11))$DEU)^2

# Lag the variable:
#german_shock <- lag(german_shock)

# And now, I use them as an exogenous regressor for the other countries.

#spec_vol_spill <- ugarchspec(mean.model = list(armaOrder = c(4,0)), variance.model = list(garchOrder = c(1, 1)), distribution.model = "norm")

#multispec_vol_spill <- multispec(replicate(n_countries, spec_vol_spill))
#dcc_vol_spill <- dccspec(multispec_vol_spill, dccOrder = c(0,1))

# Then estimate the conditional variance with the lagged variance as exogenous regressor:
#dccfit_vol_spill <- dccfit(dcc_vol_spill, df_ts) 
#dccfit_vol_spill@model$varmodel
#coef(dccfit_vol_spill)
```




# Results

---
## Conditional mean for some country

## Conditional variance for some country
---

## Conditional correlations
```{r extract_data}
# Extract the data to a format we can easily plot:
corrs <- rcor(dcc.fit)
corrs[,,dim(corrs)[3]] # Inspect last value

gen_dcc_data <- function(base_country){
  # Function generates a data frame of correlations of one country with all others
 ix <- 1
 corr_df <- data.frame(corrs[base_country,base_country,])
 for (country in countries) {
   ix <- ix +1
   corr_df <- cbind(corr_df, corrs[base_country,country,])
   }
 names(corr_df) <- c("base", countries)
 corr_df <- corr_df %>% select(-base) # drop the initialising column
 # To access the date variable, convert to xts and then back to df
 corr_df <- fortify(as.xts(corr_df))
 # Gather to enable easy plotting
 corr_df <- corr_df %>% gather(key = "country", value = "correlation", countries)
 return(corr_df)
}

# Plot data
plot_dcc_data <- function(country_ix){
  df <- gen_dcc_data(country_ix)
  p <- df %>% ggplot(aes(x = Index, y = correlation, color = country)) + 
    geom_line() + coord_cartesian(ylim = c(-0.1, 0.5)) +
    labs(subtitle = countries[country_ix]) +
    theme(legend.position = "none")
  return(p)
}

corr_plots <- lapply(1:length(countries), plot_dcc_data)


```

```{r, fig.height = 10, fig.width = 7}
corr_plots[[1]] / corr_plots[[2]] / corr_plots[[3]] / corr_plots[[4]] / corr_plots[[5]] + theme(legend.position = "bottom")
```

---
## Conditional correlation forecast
---

```{r}
# Plot average
avg_correlation <- function(country_list){
  # TODO: how to define this properly?
  # E.g. includes correlation with itself I think - simply subtract this 
  sum_cor <- gen_dcc_data(1)
  sum_cor$correlation <- rep(0, nrow(sum_cor))
  for (c in country_list){
    sum_cor$correlation <- sum_cor$correlation + gen_dcc_data(c)$correlation
  }
  avg_cor <- sum_cor
  avg_cor$correlation <- sum_cor$correlation/ length(country_list)
  
  return(avg_cor)
}
#avg_cor_ts <- avg_correlation(countries)
#as_tibble(avg_cor_ts) %>% ggplot(aes(x = Index, y = correlation, color = country)) + geom_line()

```


## Table of DCC GARCH estimated results
* Conditional mean
* Conditional variance



# Discussion of relevant periods

---
# Notes
## ARIMA models (notes)

ARIMA models: Box Jenkins approach, slide 30. Not a big problem of portmanteau rejecting the null of white noise. No problem as the model selection was based on training, next we just care about how good the model is out of sample.

data analysis icluding original discussion of a data set with a clear final goal

# Analysis notes
This analysis was inspired by: https://ro.uow.edu.au/cgi/viewcontent.cgi?article=1280&context=aabfj
https://www.jstor.org/stable/pdf/27647202.pdf?refreqid=excelsior%3Ada7c65572902730eb32916bc6ff4825c

Explanatory paper of the library rmgarch by the author: https://cran.r-project.org/web/packages/rmgarch/vignettes/The_rmgarch_models.pdf

Paper on gdp synchroisation:
https://ec.europa.eu/eurostat/documents/3888793/7572028/KS-TC-16-010-EN-N.pdf/969fe12f-cc19-4447-81f0-a5c95265798a

A paper using DCC on turkey for finding synchronisation between GDP and stock marketets: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=Analyzing+the+synchronization+between+the+financial+and+business+cycles+in+Turkey&btnG=

Stock Watson 2005 is authoriative: UNDERSTANDING CHANGES IN INTERNATIONAL BUSINESS CYCLE DYNAMICS
P. 974 about the data used to analyse volatitlity:  "Let yt be the quarterly GDP growth
at an annual rate". They look att individual countries and not conditional correlation between countries.

---

# References
